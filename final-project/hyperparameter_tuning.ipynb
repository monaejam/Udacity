{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive\n",
        "\n",
        "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Environment\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "import os\n",
        "import shutil"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726445319108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Specify the name of the compute cluster\n",
        "compute_name = \"cpu-cluster\"\n",
        "\n",
        "# Check if the compute target already exists, otherwise create it\n",
        "try:\n",
        "    trainCluster = ComputeTarget(ws, compute_name)\n",
        "    print(f\"{compute_name} exists already\")\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_D2_V2\", max_nodes=2)\n",
        "    trainCluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
        "    trainCluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cpu-cluster exists already\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726445323106
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a new environment\n",
        "env = Environment(name=\"my-sklearn-env\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_conda_package(\"scikit-learn\")\n",
        "conda_dep.add_conda_package(\"pandas\")  # Add pandas dependency\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Register the environment\n",
        "env.register(workspace=ws)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "{\n    \"assetId\": \"azureml://locations/northcentralus/workspaces/14469e3d-16d9-46f1-b40b-c8dfbce3c1b7/environments/my-sklearn-env/versions/1\",\n    \"databricks\": {\n        \"eggLibraries\": [],\n        \"jarLibraries\": [],\n        \"mavenLibraries\": [],\n        \"pypiLibraries\": [],\n        \"rcranLibraries\": []\n    },\n    \"docker\": {\n        \"arguments\": [],\n        \"baseDockerfile\": null,\n        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20240908.v1\",\n        \"baseImageRegistry\": {\n            \"address\": null,\n            \"password\": null,\n            \"registryIdentity\": null,\n            \"username\": null\n        },\n        \"buildContext\": null,\n        \"enabled\": false,\n        \"platform\": {\n            \"architecture\": \"amd64\",\n            \"os\": \"Linux\"\n        },\n        \"sharedVolumes\": true,\n        \"shmSize\": null\n    },\n    \"environmentVariables\": {\n        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n    },\n    \"inferencingStackVersion\": null,\n    \"name\": \"my-sklearn-env\",\n    \"python\": {\n        \"baseCondaEnvironment\": null,\n        \"condaDependencies\": {\n            \"channels\": [\n                \"anaconda\",\n                \"conda-forge\"\n            ],\n            \"dependencies\": [\n                \"python=3.8.13\",\n                {\n                    \"pip\": [\n                        \"azureml-defaults\"\n                    ]\n                },\n                \"scikit-learn\",\n                \"pandas\"\n            ],\n            \"name\": \"project_environment\"\n        },\n        \"condaDependenciesFile\": null,\n        \"interpreterPath\": \"python\",\n        \"userManagedDependencies\": false\n    },\n    \"r\": null,\n    \"spark\": {\n        \"packages\": [],\n        \"precachePackages\": true,\n        \"repositories\": []\n    },\n    \"version\": \"1\"\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726445327098
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "# Replace with your GitHub token and the correct URL\n",
        "GITHUB_TOKEN = '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'\n",
        "url = 'https://raw.githubusercontent.com/monaejam/Udacity/main/capston/heart_failure_clinical_records_dataset.csv'\n",
        "\n",
        "# Fetch the raw CSV content from GitHub\n",
        "headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = StringIO(response.text)\n",
        "    df = pd.read_csv(data)\n",
        "    \n",
        "    # Print first few rows to confirm data retrieval\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f\"Failed to retrieve file. Status code: {response.status_code}\")\n",
        "\n",
        "# Now, you can proceed to register this dataset in your Azure ML Workspace\n",
        "\n",
        "# Connect to your workspace\n",
        "ws = Workspace.from_config()  # Or provide parameters manually if needed\n",
        "\n",
        "# Register the dataset in Azure ML Workspace\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(df, target=ws.get_default_datastore(), name=\"heart_failure_dataset\")\n",
        "\n",
        "# Convert to pandas DataFrame and describe the dataset\n",
        "df = dataset.to_pandas_dataframe()\n",
        "df.describe()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n0  75.0        0                       582         0                 20   \n1  55.0        0                      7861         0                 38   \n2  65.0        0                       146         0                 20   \n3  50.0        1                       111         0                 20   \n4  65.0        1                       160         1                 20   \n\n   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n0                    1  265000.00               1.9           130    1   \n1                    0  263358.03               1.1           136    1   \n2                    0  162000.00               1.3           129    1   \n3                    0  210000.00               1.9           137    1   \n4                    0  327000.00               2.7           116    0   \n\n   smoking  time  DEATH_EVENT  \n0        0     4            1  \n1        0     6            1  \n2        1     7            1  \n3        0     7            1  \n4        0     8            1  \nValidating arguments.\nArguments validated.\nValidating arguments.\nArguments validated.\n'overwrite' is set to True. Any file already present in the target will be overwritten.\nUploading files from '/tmp/tmp_dizx6y4' to 'managed-dataset/e5f94448-203f-4ced-a8bf-645a9ff5ed4c/'\nCopying 1 files with concurrency set to 1\nCopied /tmp/tmp_dizx6y4/dataframe.parquet, file 1 out of 1. Destination path: https://udacity10296494136.blob.core.windows.net/azureml-blobstore-14469e3d-16d9-46f1-b40b-c8dfbce3c1b7/managed-dataset/e5f94448-203f-4ced-a8bf-645a9ff5ed4c/dataframe.parquet\nFiles copied=1, skipped=0, failed=0\nSuccessfully uploaded file to datastore.\nCreating and registering a new dataset.\nSuccessfully created and registered a new dataset.\n{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe', 'activityApp': 'TabularDataset'}\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "              age     anaemia  creatinine_phosphokinase    diabetes  \\\ncount  299.000000  299.000000                299.000000  299.000000   \nmean    60.833893    0.431438                581.839465    0.418060   \nstd     11.894809    0.496107                970.287881    0.494067   \nmin     40.000000    0.000000                 23.000000    0.000000   \n25%     51.000000    0.000000                116.500000    0.000000   \n50%     60.000000    0.000000                250.000000    0.000000   \n75%     70.000000    1.000000                582.000000    1.000000   \nmax     95.000000    1.000000               7861.000000    1.000000   \n\n       ejection_fraction  high_blood_pressure      platelets  \\\ncount         299.000000           299.000000     299.000000   \nmean           38.083612             0.351171  263358.029264   \nstd            11.834841             0.478136   97804.236869   \nmin            14.000000             0.000000   25100.000000   \n25%            30.000000             0.000000  212500.000000   \n50%            38.000000             0.000000  262000.000000   \n75%            45.000000             1.000000  303500.000000   \nmax            80.000000             1.000000  850000.000000   \n\n       serum_creatinine  serum_sodium         sex    smoking        time  \\\ncount         299.00000    299.000000  299.000000  299.00000  299.000000   \nmean            1.39388    136.625418    0.648829    0.32107  130.260870   \nstd             1.03451      4.412477    0.478136    0.46767   77.614208   \nmin             0.50000    113.000000    0.000000    0.00000    4.000000   \n25%             0.90000    134.000000    0.000000    0.00000   73.000000   \n50%             1.10000    137.000000    1.000000    0.00000  115.000000   \n75%             1.40000    140.000000    1.000000    1.00000  203.000000   \nmax             9.40000    148.000000    1.000000    1.00000  285.000000   \n\n       DEATH_EVENT  \ncount    299.00000  \nmean       0.32107  \nstd        0.46767  \nmin        0.00000  \n25%        0.00000  \n50%        0.00000  \n75%        1.00000  \nmax        1.00000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>anaemia</th>\n      <th>creatinine_phosphokinase</th>\n      <th>diabetes</th>\n      <th>ejection_fraction</th>\n      <th>high_blood_pressure</th>\n      <th>platelets</th>\n      <th>serum_creatinine</th>\n      <th>serum_sodium</th>\n      <th>sex</th>\n      <th>smoking</th>\n      <th>time</th>\n      <th>DEATH_EVENT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.00000</td>\n      <td>299.000000</td>\n      <td>299.000000</td>\n      <td>299.00000</td>\n      <td>299.000000</td>\n      <td>299.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>60.833893</td>\n      <td>0.431438</td>\n      <td>581.839465</td>\n      <td>0.418060</td>\n      <td>38.083612</td>\n      <td>0.351171</td>\n      <td>263358.029264</td>\n      <td>1.39388</td>\n      <td>136.625418</td>\n      <td>0.648829</td>\n      <td>0.32107</td>\n      <td>130.260870</td>\n      <td>0.32107</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>11.894809</td>\n      <td>0.496107</td>\n      <td>970.287881</td>\n      <td>0.494067</td>\n      <td>11.834841</td>\n      <td>0.478136</td>\n      <td>97804.236869</td>\n      <td>1.03451</td>\n      <td>4.412477</td>\n      <td>0.478136</td>\n      <td>0.46767</td>\n      <td>77.614208</td>\n      <td>0.46767</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>23.000000</td>\n      <td>0.000000</td>\n      <td>14.000000</td>\n      <td>0.000000</td>\n      <td>25100.000000</td>\n      <td>0.50000</td>\n      <td>113.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>4.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>116.500000</td>\n      <td>0.000000</td>\n      <td>30.000000</td>\n      <td>0.000000</td>\n      <td>212500.000000</td>\n      <td>0.90000</td>\n      <td>134.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>73.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>60.000000</td>\n      <td>0.000000</td>\n      <td>250.000000</td>\n      <td>0.000000</td>\n      <td>38.000000</td>\n      <td>0.000000</td>\n      <td>262000.000000</td>\n      <td>1.10000</td>\n      <td>137.000000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>115.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>70.000000</td>\n      <td>1.000000</td>\n      <td>582.000000</td>\n      <td>1.000000</td>\n      <td>45.000000</td>\n      <td>1.000000</td>\n      <td>303500.000000</td>\n      <td>1.40000</td>\n      <td>140.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>203.000000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>95.000000</td>\n      <td>1.000000</td>\n      <td>7861.000000</td>\n      <td>1.000000</td>\n      <td>80.000000</td>\n      <td>1.000000</td>\n      <td>850000.000000</td>\n      <td>9.40000</td>\n      <td>148.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>285.000000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726445332087
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperdrive Configuration\n",
        "\n",
        "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598531923519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify parameter sampler\n",
        "ps = RandomParameterSampling(\n",
        "    {\n",
        "        \"--C\": choice(1, 2, 3, 4, 5),\n",
        "        \"--max_iter\": choice(80, 100, 120, 150, 170, 200)\n",
        "    }\n",
        ")\n",
        "\n",
        "# Specify a Policy\n",
        "policy = BanditPolicy(evaluation_interval=1, slack_factor=0.2, delay_evaluation=5)\n",
        "\n",
        "# Create a directory for training and copy the training script\n",
        "if \"training\" not in os.listdir():\n",
        "    os.mkdir(\"./training\")\n",
        "shutil.copy('train.py', './training')\n",
        "\n",
        "# Create a ScriptRunConfig\n",
        "src = ScriptRunConfig(source_directory='./',\n",
        "                      script='train.py',\n",
        "                      arguments=[\n",
        "                          '--data-folder', dataset.as_named_input('input')],\n",
        "                      compute_target=trainCluster,\n",
        "                      environment=env)\n",
        "\n",
        "\n",
        "# Create a HyperDriveConfig using the ScriptRunConfig, hyperparameter sampler, and policy.\n",
        "hyperdrive_config = HyperDriveConfig(hyperparameter_sampling=ps,\n",
        "                                     primary_metric_name='Accuracy',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                     policy=policy,\n",
        "                                     run_config=src,\n",
        "                                     max_concurrent_runs=2,\n",
        "                                     max_total_runs=10,                                     \n",
        "                                    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Details\n",
        "\n",
        "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
        "\n",
        "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598544898497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "\n",
        "# Connect to your workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Get the default datastore (you can also create or specify another datastore)\n",
        "datastore = Datastore.get(ws, 'workspaceblobstore')  # or ws.get_default_datastore()\n",
        "\n",
        "# Upload the dataset to the datastore\n",
        "datastore.upload_files(\n",
        "    ['./heart_failure_clinical_records_dataset.csv'],  # List of local file paths\n",
        "    target_path='datasets/',  # Destination path in the datastore\n",
        "    overwrite=True  # Overwrite existing files\n",
        ")\n",
        "\n",
        "print(\"Dataset uploaded to datastore.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 1 files\nUploading ./heart_failure_clinical_records_dataset.csv\nUploaded ./heart_failure_clinical_records_dataset.csv, 1 files out of an estimated total of 1\nUploaded 1 files\nDataset uploaded to datastore.\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726443672060
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'datasets/heart_failure_clinical_records_dataset.csv')])\n",
        "\n",
        "# Register as a new version\n",
        "dataset = dataset.register(workspace=ws, name='heart_failure_dataset', create_new_version=True)\n",
        "\n",
        "print(\"Dataset registered as a new version.\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset registered as a new version.\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726443675125
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset, Workspace\n",
        "\n",
        "# Connect to your Azure ML workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Load the existing dataset by name\n",
        "dataset = Dataset.get_by_name(workspace=ws, name='heart_failure_dataset')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_pandas_dataframe', 'activityApp': 'TabularDataset'}\nDataset loaded successfully.\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726443017299
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify parameter sampler\n",
        "ps = RandomParameterSampling(\n",
        "    {\n",
        "        \"--C\": choice(1, 2, 3, 4, 5),\n",
        "        \"--max_iter\": choice(80, 100, 120, 150, 170, 200)\n",
        "    }\n",
        ")\n",
        "\n",
        "# Specify a Policy\n",
        "# Define the policy\n",
        "policy = BanditPolicy(evaluation_interval=1, slack_factor=0.2, delay_evaluation=5)\n",
        "\n",
        "# Create a directory for training and copy the training script if not already existing\n",
        "if \"training\" not in os.listdir():\n",
        "    os.mkdir(\"./training\")\n",
        "shutil.copy('train.py', './training')\n",
        "\n",
        "# Create a ScriptRunConfig without passing the --data-folder argument\n",
        "src = ScriptRunConfig(source_directory='./',\n",
        "                      script='train.py',\n",
        "                      compute_target=trainCluster,\n",
        "                      environment=env)\n",
        "\n",
        "\n",
        "# Create a HyperDriveConfig using the ScriptRunConfig, hyperparameter sampler, and policy.\n",
        "hyperdrive_config = HyperDriveConfig(hyperparameter_sampling=ps,\n",
        "                                     primary_metric_name='Accuracy',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                     policy=policy,\n",
        "                                     run_config=src,\n",
        "                                     max_concurrent_runs=2,\n",
        "                                     max_total_runs=10,                                     \n",
        "                                    )"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1726446292187
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**submit experiment and best run**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the HyperDrive run\n",
        "experiment_name = 'hyper-exp'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "hyperdrive_run = experiment.submit(config=hyperdrive_config)\n",
        "\n",
        "# wait for completion and retrieve the best run\n",
        "hyperdrive_run.wait_for_completion(show_output=True)\n",
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "\n",
        "print('Best Run Id: ', best_run.id)\n",
        "print('\\nAccuracy:', best_run_metrics['Accuracy'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a\nWeb View: https://ml.azure.com/runs/HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a?wsid=/subscriptions/d2d90bd8-e567-4097-88c9-9532cc375686/resourcegroups/cloud_shell/workspaces/Udacity_1&tid=f3822f31-4d32-4719-a061-c45fac0a64ab\n\nStreaming azureml-logs/hyperdrive.txt\n=====================================\n\n[2024-09-16T00:25:00.530352][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\n[2024-09-16T00:25:00.9425732Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_0' \n[2024-09-16T00:25:01.0524194Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_1' \n[2024-09-16T00:25:01.013217][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:25:01.4640758Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_1' \n[2024-09-16T00:25:01.5575820Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_0' \n[2024-09-16T00:26:30.132848][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:26:30.4357460Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2' \n[2024-09-16T00:26:30.375765][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:26:30.9147867Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2' \n[2024-09-16T00:27:30.161054][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:27:30.4931866Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_3' \n[2024-09-16T00:27:30.453191][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:27:30.8578753Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_3' \n[2024-09-16T00:28:00.150937][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:28:00.5261292Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_4' \n[2024-09-16T00:28:00.474725][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:28:00.9141426Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_4' \n[2024-09-16T00:29:00.143594][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:29:00.4389795Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_5' \n[2024-09-16T00:29:00.397423][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:29:00.8063765Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_5' \n[2024-09-16T00:29:30.168690][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:29:30.5358247Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_6' \n[2024-09-16T00:29:30.491877][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:29:31.0242429Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_6' \n[2024-09-16T00:30:30.182380][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:30:30.5721451Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_7' \n[2024-09-16T00:30:30.546774][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:30:31.1189741Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_7' \n[2024-09-16T00:31:00.225476][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\n[2024-09-16T00:31:00.5361801Z][SCHEDULER][INFO]Scheduling job, id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_8' \n[2024-09-16T00:31:00.488564][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\n[2024-09-16T00:31:01.0310912Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_8' \n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726446284410
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "print(\"best run details :\",best_run.get_details())\n",
        "print(\"best run file names :\",best_run.get_file_names())\n",
        "print(\"best run metrics :\",best_run.get_metrics())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "best run details : {'runId': 'HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2024-09-16T00:26:58.924463Z', 'endTimeUtc': '2024-09-16T00:27:21.671315Z', 'services': {}, 'properties': {'_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': 'cpu-cluster', 'ContentSnapshotId': '5fdff818-d85b-402a-9dad-e09b0e266cf8', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '0cb1973b-83a4-412e-bfb9-956ff3593d33'}, 'consumptionDetails': {'type': 'Reference'}}], 'outputDatasets': [], 'runDefinition': {'script': 'train.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--C', '5', '--max_iter', '80'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': 2592000, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'AzureML-Tutorial', 'version': '88', 'assetId': 'azureml://registries/azureml/environments/AzureML-Tutorial/versions/88', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-core==1.37.0.post1', 'azureml-defaults==1.37.0', 'azureml-telemetry==1.37.0', 'azureml-train-restclients-hyperdrive==1.37.0', 'azureml-train-core==1.37.0', 'azureml-widgets==1.37.0', 'azureml-pipeline-core==1.37.0', 'azureml-pipeline-steps==1.37.0', 'azureml-opendatasets==1.37.0', 'azureml-automl-core==1.37.0', 'azureml-automl-runtime==1.37.0', 'azureml-train-automl-client==1.37.0', 'azureml-train-automl-runtime==1.37.0', 'azureml-train-automl==1.37.0', 'azureml-train==1.37.0', 'azureml-sdk==1.37.0', 'azureml-interpret==1.37.0', 'azureml-tensorboard==1.37.0', 'azureml-mlflow==1.37.0', 'mlflow', 'sklearn-pandas']}, 'pandas', 'numpy', 'tqdm', 'scikit-learn', 'matplotlib'], 'name': 'azureml_1296d9ccb6d6509a0126eeef4e26fcc9'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210507.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': None}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/backgroundProcess.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=qcpWK7BIDx60xfPEEi6u7OYgUVh9EoXc4gb3415el5A%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A23%3A34Z&se=2024-09-16T08%3A33%3A34Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=DVos19a3WQeqnxEVwUb0kFf3owLxtBPxIaWoKKERjSw%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A23%3A34Z&se=2024-09-16T08%3A33%3A34Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=TPkjz%2BkfmnNtaBlEBiuXSnRE27IUdOKCPtp1Vd6fSUU%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A23%3A34Z&se=2024-09-16T08%3A33%3A34Z&sp=r', 'user_logs/std_log.txt': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=zSep753Z1M2dJTZOiBiyk%2BobjJA6Eqe4D3Mv32aqUTA%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A21%3A49Z&ske=2024-09-17T00%3A31%3A49Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=0EoU2n3MwLmOYuIpuj4FqGEVwCMZvrQJg6%2FXkhNvUuI%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=8VvbzYWl6EDmREpmy240gLNE3eL%2FBeXS1gRVOyNDh4c%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=32ZopgQTXpFi5HRtQsJyHu7uTMqq%2BcaVlA5XzGsMF%2FU%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=xC7JvnEeUB2VHAb05iYTgarY0Uh37DktMUX06sNqqRc%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=t1%2FJXQLaw7hzwt8fs0naygIGq5bOcZ4f3IBliyuTGcI%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://udacity10296494136.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=bAb7a4qeFTDK518hmCi2oGm8rMlu8xyRolYcHh58n7k%3D&skoid=09d38778-a778-4ec5-a7b5-240fbc571e5c&sktid=f3822f31-4d32-4719-a061-c45fac0a64ab&skt=2024-09-16T00%3A22%3A44Z&ske=2024-09-17T00%3A32%3A44Z&sks=b&skv=2019-07-07&st=2024-09-16T00%3A24%3A13Z&se=2024-09-16T08%3A34%3A13Z&sp=r'}, 'submittedBy': 'mona jam'}\nbest run file names : ['logs/azureml/dataprep/backgroundProcess.log', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log', 'logs/azureml/dataprep/rslex.log', 'outputs/hyper-model.pkl', 'system_logs/cs_capability/cs-capability.log', 'system_logs/hosttools_capability/hosttools-capability.log', 'system_logs/lifecycler/execution-wrapper.log', 'system_logs/lifecycler/lifecycler.log', 'system_logs/metrics_capability/metrics-capability.log', 'system_logs/snapshot_capability/snapshot-capability.log', 'user_logs/std_log.txt']\nbest run metrics : {'Regularization Strength': 5.0, 'Max iterations': 80, 'Accuracy': 0.7333333333333333}\n"
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726446853745
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_run.register_model(model_name = \"hyperdrive_best_run.pkl\", model_path = './outputs/')\n",
        "\n",
        "print(best_run)\n",
        "best_run.download_file( name= './outputs/hyper-model.pkl')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run(Experiment: hyper-exp,\nId: HD_2e4d3d56-e61a-43c6-97bd-64e0538cf13a_2,\nType: azureml.scriptrun,\nStatus: Completed)\n"
        }
      ],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726446869320
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Checklist**\n",
        "- I have registered the model.\n",
        "- I have deployed the model with the best accuracy as a webservice.\n",
        "- I have tested the webservice by sending a request to the model endpoint.\n",
        "- I have deleted the webservice and shutdown all the computes that I have used.\n",
        "- I have taken a screenshot showing the model endpoint as active.\n",
        "- The project includes a file containing the environment details.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}